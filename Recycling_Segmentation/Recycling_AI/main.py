from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from PIL import Image, ImageDraw, ImageFont, ImageEnhance
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import cv2
import io
import os
import uvicorn
import base64
import time

# ===== FastAPI ì•± ìƒì„± =====
app = FastAPI(title="Real-time Recycling Segmentation API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {
        "message": "Real-time Recycling Segmentation API is running!",
        "version": "8.0",
        "features": ["pytorch_weights", "custom_model", "real_time_optimization"]
    }

@app.get("/health")
async def health():
    return {"status": "healthy", "gpu_available": torch.cuda.is_available()}

# ===== ëª¨ë¸ ê´€ë ¨ ì„¤ì • =====
# weights.pt íŒŒì¼ì—ì„œ ì§ì ‘ ëª¨ë¸ ë¡œë“œ

# ===== ëª¨ë¸ ë¡œë“œ =====
WEIGHTS_PATH = os.path.join(os.path.dirname(__file__), "weights.pt")

try:
    print("ğŸ¤– PyTorch ëª¨ë¸ ë¡œë“œ ì¤‘...")
    
    if not os.path.exists(WEIGHTS_PATH):
        raise FileNotFoundError(f"weights.pt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {WEIGHTS_PATH}")
    
    # ëª¨ë¸ ì§ì ‘ ë¡œë“œ (ì™„ì „í•œ ëª¨ë¸ì´ ì €ì¥ëœ ê²½ìš°)
    model = torch.load(WEIGHTS_PATH, map_location='cpu')
    print(f"âœ… ëª¨ë¸ íŒŒì¼ ë¡œë“œ ì™„ë£Œ!")
    
    # ëª¨ë¸ íƒ€ì… í™•ì¸
    print(f"   ëª¨ë¸ íƒ€ì…: {type(model)}")
    
    # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    model.eval()
    
    # GPU ìµœì í™”
    if torch.cuda.is_available():
        model = model.cuda()
        print("âœ… GPU ìµœì í™” ì™„ë£Œ!")
    else:
        print("âš ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("ğŸ’¡ ê°€ëŠ¥í•œ í•´ê²°ì±…:")
    print("   1. weights.pt íŒŒì¼ì´ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸")
    print("   2. PyTorch ë²„ì „ í˜¸í™˜ì„± í™•ì¸")
    print("   3. íŒŒì¼ì´ ì†ìƒë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸")
    model = None

# ===== ì„¤ì • =====
class_names = ["background", "can", "glass", "paper", "plastic", "styrofoam", "vinyl"]

class_colors_bright = [
    None,             # background - íˆ¬ëª…
    (255, 69, 0),     # can - ì„ ëª…í•œ ì£¼í™©
    (50, 205, 50),    # glass - ë°ì€ ì´ˆë¡
    (30, 144, 255),   # paper - ë°ì€ íŒŒë‘
    (255, 20, 147),   # plastic - í•«í•‘í¬
    (255, 215, 0),    # styrofoam - ê³¨ë“œ
    (138, 43, 226)    # vinyl - ë°”ì´ì˜¬ë ›
]

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {device}")

font_path = os.path.join(os.path.dirname(__file__), "Pretendard-SemiBold.otf")

# ì‹¤ì‹œê°„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„¤ì •
MODEL_INPUT_SIZE = 512  # ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” í¬ê¸°ì— ë§ê²Œ ì¡°ì •
PROCESSING_TIMEOUT = 10.0  # ìŠ¤ë§ˆíŠ¸í° ê³ í•´ìƒë„ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œê°„ ê³ ë ¤

# ===== ì „ì²˜ë¦¬ í•¨ìˆ˜ =====

def smart_preprocess_image(image, target_size=MODEL_INPUT_SIZE):
    """ìŠ¤ë§ˆíŠ¸í° ì‚¬ì§„ì„ ìœ„í•œ ì§€ëŠ¥í˜• ì „ì²˜ë¦¬"""
    start_time = time.time()
    original_size = image.size
    width, height = original_size
    
    print(f"   ì›ë³¸ í¬ê¸°: {width}x{height}")
    
    # 1ë‹¨ê³„: ë„ˆë¬´ í° ì´ë¯¸ì§€ëŠ” ë¨¼ì € ì¶•ì†Œ (ë©”ëª¨ë¦¬/ì†ë„ ìµœì í™”)
    max_dimension = 2048
    if max(width, height) > max_dimension:
        scale = max_dimension / max(width, height)
        new_width = int(width * scale)
        new_height = int(height * scale)
        image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
        width, height = new_width, new_height
        print(f"   1ì°¨ ì¶•ì†Œ: {width}x{height}")
    
    # 2ë‹¨ê³„: ìŠ¤ë§ˆíŠ¸ í¬ë¡­ (ì¤‘ì•™ + ê°ì²´ ì¤‘ì‹¬)
    # ë” ì‘ì€ ì°¨ì›ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ì‚¬ê°í˜• í¬ë¡­
    crop_size = min(width, height)
    
    # ì¤‘ì•™ í¬ë¡­ (ê¸°ë³¸)
    left = (width - crop_size) // 2
    top = (height - crop_size) // 2
    
    # ìŠ¤ë§ˆíŠ¸í° ì‚¬ì§„ íŠ¹ì„± ê³ ë ¤í•œ ì¡°ì •
    if height > width:  # ì„¸ë¡œ ì‚¬ì§„ (ì¼ë°˜ì ì¸ ìŠ¤ë§ˆíŠ¸í° ì‚¬ì§„)
        # ìœ„ìª½ìœ¼ë¡œ ì•½ê°„ ì¹˜ìš°ì¹˜ê²Œ (í…Œì´ë¸” ìœ„ ë¬¼ê±´ ì´¬ì˜ ê³ ë ¤)
        top = max(0, top - crop_size // 6)
    
    cropped = image.crop((left, top, left + crop_size, top + crop_size))
    print(f"   í¬ë¡­ ì™„ë£Œ: {crop_size}x{crop_size}")
    
    # 3ë‹¨ê³„: ëª¨ë¸ ì…ë ¥ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
    resized = cropped.resize((target_size, target_size), Image.Resampling.LANCZOS)
    
    # 4ë‹¨ê³„: ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ (ì„ íƒì )
    enhanced = enhance_image_quality(resized)
    
    # 5ë‹¨ê³„: í…ì„œ ë³€í™˜
    img_array = np.array(enhanced).astype(np.float32) / 255.0
    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
    
    if torch.cuda.is_available():
        img_tensor = img_tensor.cuda()
    
    elapsed = time.time() - start_time
    print(f"   ìŠ¤ë§ˆíŠ¸ ì „ì²˜ë¦¬ ì™„ë£Œ: {elapsed:.3f}ì´ˆ")
    
    return img_tensor, original_size

def enhance_image_quality(image):
    """ìŠ¤ë§ˆíŠ¸í° ì‚¬ì§„ í’ˆì§ˆ í–¥ìƒ"""
    # ëŒ€ë¹„ ë° ì„ ëª…ë„ í–¥ìƒ (ë¶„ë¦¬ìˆ˜ê±° ë¬¼í’ˆ êµ¬ë¶„ì— ë„ì›€)
    
    # ëŒ€ë¹„ í–¥ìƒ
    contrast_enhancer = ImageEnhance.Contrast(image)
    image = contrast_enhancer.enhance(1.2)
    
    # ì„ ëª…ë„ í–¥ìƒ
    sharpness_enhancer = ImageEnhance.Sharpness(image)
    image = sharpness_enhancer.enhance(1.1)
    
    # ì±„ë„ ì•½ê°„ í–¥ìƒ (í”Œë¼ìŠ¤í‹±, ìº” ë“± ìƒ‰ìƒ êµ¬ë¶„ì— ë„ì›€)
    color_enhancer = ImageEnhance.Color(image)
    image = color_enhancer.enhance(1.1)
    
    return image

def preprocess_image(image, target_size=MODEL_INPUT_SIZE):
    """ê¸°ì¡´ í•¨ìˆ˜ í˜¸í™˜ì„± ìœ ì§€"""
    return smart_preprocess_image(image, target_size)

# ===== ëª¨ë¸ ì˜ˆì¸¡ =====

def predict_segmentation(image_tensor):
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì˜ˆì¸¡"""
    start_time = time.time()
    
    with torch.no_grad():
        # ëª¨ë¸ ì˜ˆì¸¡ (ì¶œë ¥ í˜•íƒœëŠ” ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
        outputs = model(image_tensor)
        
        # ì¶œë ¥ì´ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í…ì„œì¸ì§€ í™•ì¸
        if isinstance(outputs, dict):
            if "logits" in outputs:
                logits = outputs["logits"]
            elif "prediction" in outputs:
                logits = outputs["prediction"]
            else:
                # ì²« ë²ˆì§¸ ê°’ì„ logitsë¡œ ê°€ì •
                logits = list(outputs.values())[0]
        else:
            # ì§ì ‘ í…ì„œì¸ ê²½ìš°
            logits = outputs
        
        # ì†Œí”„íŠ¸ë§¥ìŠ¤ ì ìš©
        probs = F.softmax(logits, dim=1)[0].cpu().numpy()
        prediction = np.argmax(probs, axis=0)
        confidence_map = np.max(probs, axis=0)
        
        print(f"   ì˜ˆì¸¡ ì™„ë£Œ - Shape: {prediction.shape}")
        print(f"   ê°ì§€ëœ í´ë˜ìŠ¤: {np.unique(prediction)}")
    
    elapsed = time.time() - start_time
    print(f"   ëª¨ë¸ ì˜ˆì¸¡: {elapsed:.3f}ì´ˆ")
    
    return probs, prediction, confidence_map

# ===== í›„ì²˜ë¦¬ =====

def postprocess_prediction(prediction, confidence_map, confidence_threshold=0.3):
    """ì˜ˆì¸¡ ê²°ê³¼ í›„ì²˜ë¦¬"""
    start_time = time.time()
    
    # ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§
    mask = np.where(confidence_map >= confidence_threshold, prediction, 0)
    
    # Morphological operationsë¡œ ë…¸ì´ì¦ˆ ì œê±°
    cleaned_mask = np.zeros_like(mask)
    
    for class_id in range(1, len(class_names)):
        class_mask = (mask == class_id).astype(np.uint8)
        
        if np.sum(class_mask) == 0:
            continue
        
        # ë…¸ì´ì¦ˆ ì œê±°
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        opened = cv2.morphologyEx(class_mask, cv2.MORPH_OPEN, kernel)
        
        # ì‘ì€ ì˜ì—­ ì œê±°
        if np.sum(opened) >= 50:
            cleaned_mask[opened > 0] = class_id
    
    elapsed = time.time() - start_time
    print(f"   í›„ì²˜ë¦¬: {elapsed:.3f}ì´ˆ")
    
    return cleaned_mask

# ===== ì‹œê°í™” =====

def create_visualization(image, mask):
    """ì‹œê°í™” ìƒì„±"""
    start_time = time.time()
    
    img_np = np.array(image)
    
    # OVERLAY ìƒì„±
    overlay = img_np.copy().astype(np.float32)
    
    for class_id in range(1, len(class_names)):
        class_region = (mask == class_id)
        if np.any(class_region):
            color = class_colors_bright[class_id]
            overlay[class_region] = (
                img_np[class_region].astype(np.float32) * 0.6 +
                np.array(color) * 0.4
            )
    
    overlay = np.clip(overlay, 0, 255).astype(np.uint8)
    
    # PREDICT ìƒì„±
    predict = np.zeros_like(img_np)
    
    for class_id in range(1, len(class_names)):
        class_region = (mask == class_id)
        if np.any(class_region):
            predict[class_region] = class_colors_bright[class_id]
    
    # ë¼ë²¨ ì¶”ê°€
    overlay_pil = add_labels(Image.fromarray(overlay), mask)
    predict_pil = add_labels(Image.fromarray(predict), mask)
    
    elapsed = time.time() - start_time
    print(f"   ì‹œê°í™”: {elapsed:.3f}ì´ˆ")
    
    return predict_pil, overlay_pil

def add_labels(image, mask):
    """ë¼ë²¨ ì¶”ê°€"""
    draw = ImageDraw.Draw(image)
    
    try:
        font = ImageFont.truetype(font_path, 18)
    except:
        font = ImageFont.load_default()

    for class_id in range(1, len(class_names)):
        class_mask = (mask == class_id)
        if not np.any(class_mask):
            continue

        if np.sum(class_mask) < 100:  # ë„ˆë¬´ ì‘ì€ ì˜ì—­ì€ ìƒëµ
            continue
        
        y_coords, x_coords = np.where(class_mask)
        x_center = int(np.mean(x_coords))
        y_center = int(np.mean(y_coords))
        
        label = class_names[class_id]
        
        # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
        bbox = draw.textbbox((0, 0), label, font=font)
        text_w = bbox[2] - bbox[0]
        text_h = bbox[3] - bbox[1]
        
        # ë°°ê²½ ë°•ìŠ¤
        padding = 4
        box_x1 = x_center - text_w//2 - padding
        box_y1 = y_center - text_h//2 - padding
        box_x2 = x_center + text_w//2 + padding
        box_y2 = y_center + text_h//2 + padding
        
        draw.rectangle([box_x1, box_y1, box_x2, box_y2], fill=(0, 0, 0))
        draw.text(
            (x_center - text_w//2, y_center - text_h//2), 
            label, 
            fill="white", 
            font=font
        )
        
    return image

# ===== ê²°ê³¼ ë¶„ì„ =====

def analyze_results(mask):
    """ê²°ê³¼ ë¶„ì„"""
    detected_classes = []
    total_pixels = mask.size
    
    unique, counts = np.unique(mask, return_counts=True)
    
    for class_id, pixel_count in zip(unique, counts):
        if class_id > 0 and class_id < len(class_names):
            percentage = (pixel_count / total_pixels) * 100
            
            if percentage >= 0.5:  # 0.5% ì´ìƒë§Œ
                detected_classes.append({
                    'class': class_names[class_id],
                    'pixels': int(pixel_count),
                    'percentage': round(percentage, 1)
                })
    
    detected_classes.sort(key=lambda x: x['pixels'], reverse=True)
    class_names_only = [item['class'] for item in detected_classes]
    
    return class_names_only, detected_classes

# ===== ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ =====

def process_segmentation(image_bytes):
    """ë©”ì¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬"""
    if model is None:
        raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    total_start_time = time.time()
    
    try:
        print("ğŸš€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬ ì‹œì‘...")
        
        # 1. ì´ë¯¸ì§€ ë¡œë“œ
        image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
        print(f"   ì›ë³¸: {image.size}")
        
        # 2. ì „ì²˜ë¦¬
        image_tensor, original_size = preprocess_image(image)
        
        # 3. ì˜ˆì¸¡
        probs, prediction, confidence_map = predict_segmentation(image_tensor)
        
        # 4. í›„ì²˜ë¦¬
        final_mask = postprocess_prediction(prediction, confidence_map)
        
        # 5. ê²°ê³¼ ë¶„ì„
        class_names_only, detailed_results = analyze_results(final_mask)
        
        # 6. ì‹œê°í™”
        predict_img, overlay_img = create_visualization(image, final_mask)
        
        total_elapsed = time.time() - total_start_time
        print(f"âœ… ì´ ì²˜ë¦¬ ì‹œê°„: {total_elapsed:.3f}ì´ˆ")
        print(f"âœ… ê°ì§€ ê²°ê³¼: {class_names_only}")
        
        return predict_img, overlay_img, class_names_only, total_elapsed

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

# ===== FastAPI ì—”ë“œí¬ì¸íŠ¸ =====

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰"""
    try:
        if not file.content_type.startswith("image/"):
            raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤")

        request_start = time.time()
        print(f"ğŸ“¤ ìš”ì²­ ë°›ìŒ: {file.filename}")
        
        image_bytes = await file.read()
        print(f"   íŒŒì¼ í¬ê¸°: {len(image_bytes):,} bytes")
        
        # ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬
        predict_img, overlay_img, detected_classes, processing_time = process_segmentation(image_bytes)

        # ì´ë¯¸ì§€ ì¸ì½”ë”©
        pred_bytes = io.BytesIO()
        overlay_bytes = io.BytesIO()
        
        predict_img.save(pred_bytes, format="PNG", optimize=True)
        overlay_img.save(overlay_bytes, format="PNG", optimize=True)

        # ì‘ë‹µ ìƒì„±
        if detected_classes:
            main_class = detected_classes[0]
            confidence = 0.85
            message = f"ì²˜ë¦¬ ì™„ë£Œ ({processing_time:.2f}ì´ˆ)"
        else:
            main_class = "unknown"
            confidence = 0.1
            message = f"ê°ì²´ ë¯¸ê°ì§€ ({processing_time:.2f}ì´ˆ)"

        response = {
            "prediction": base64.b64encode(pred_bytes.getvalue()).decode("utf-8"),
            "overlay": base64.b64encode(overlay_bytes.getvalue()).decode("utf-8"),
            "class": main_class,
            "confidence": confidence,
            "detected_classes": detected_classes,
            "processing_time": round(processing_time, 3),
            "status": "success",
            "message": message
        }
        
        total_time = time.time() - request_start
        print(f"ğŸ“¤ ì‘ë‹µ ì™„ë£Œ: {total_time:.3f}ì´ˆ")
        
        return response

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/predict-raw")
async def predict_raw(file: UploadFile = File(...)):
    """í˜¸í™˜ì„± ì—”ë“œí¬ì¸íŠ¸"""
    return await predict(file)

# ===== ì„œë²„ ì‹¤í–‰ =====

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    print(f"ğŸš€ ì„œë²„ ì‹œì‘: http://0.0.0.0:{port}")
    uvicorn.run("main:app", host="0.0.0.0", port=port, reload=False)